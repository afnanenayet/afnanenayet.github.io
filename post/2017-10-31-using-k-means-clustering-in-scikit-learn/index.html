<!DOCTYPE html>
<html lang="en-us">
    <head>
         
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Diagnosing livers with machine learning</title>
        
        <style>

  html body {
      font-family: 'Source Sans Pro', sans-serif;
      background-color: white;
      font-size: 150%;
  }

  :root {
      --accent: red;
      --border-width:  5px ;
  }
</style>


<link rel="stylesheet" href="https://afnan.io/css/main.css">





<link
  rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source%20Sans%20Pro"
/>
<link
  href="https://fonts.googleapis.com/css?family=Inconsolata&display=swap"
  rel="stylesheet"
/>



<link
  rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/styles/xcode.min.css">



<link
  rel="stylesheet"
  href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
  integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"
  crossorigin="anonymous"
/>


<link
  rel="stylesheet"
  href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
  integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN"
  crossorigin="anonymous"
/>
 

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js" integrity="sha256-js+I1fdbke/DJrW2qXQlrw7VUEqmdeFeOW37UC0bEiU=" crossorigin="anonymous"></script>

    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/languages/rust.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/languages/cpp.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/languages/python.min.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script>






<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.55.6" />
        

        
        
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-57800813-3"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());

          gtag('config', 'UA-57800813-3');
        </script>
        
    </head>

    
    
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

    <body>
         
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <div class="navbar-header">
                    
                        

                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse" style="float:right;white-space:nowrap">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>

                </div>

                <div class="collapse navbar-collapse">

                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="/">Home</a></li>
                            
                                <li><a href="/about/">About</a></li>
                            
                                <li><a href="/post/">Posts</a></li>
                            
                                <li><a href="/project/">Projects</a></li>
                            
                        </ul>
                    

                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:afnan@afnan.io"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/afnanenayet"><i class="fa fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://www.linkedin.com/in/afnanenayet"><i class="fa fa-linkedin"></i></a></li>
                            
                        </ul>
                    

                </div>

            </div>

        </nav>


<main>

    <div class="item">

    
    
    

    
      

    <h3><a href="/post/2017-10-31-using-k-means-clustering-in-scikit-learn/">Diagnosing livers with machine learning</a></h3>
    <h4>Infer how healthy a liver is through k-means-clustering</h4>
    
    <a href="https://afnan.iotags/machine"><kbd class="item-tag">machine</kbd></a>
    
    <a href="https://afnan.iotags/learning"><kbd class="item-tag">learning</kbd></a>
    
    <a href="https://afnan.iotags/unsupervised"><kbd class="item-tag">unsupervised</kbd></a>
    
    <a href="https://afnan.iotags/clustering"><kbd class="item-tag">clustering</kbd></a>
    
    <a href="https://afnan.iotags/scikit"><kbd class="item-tag">scikit</kbd></a>
    
    <a href="https://afnan.iotags/learn"><kbd class="item-tag">learn</kbd></a>
    
    <a href="https://afnan.iotags/sklearn"><kbd class="item-tag">sklearn</kbd></a>
    
    <a href="https://afnan.iotags/liver"><kbd class="item-tag">liver</kbd></a>
    
    <a href="https://afnan.iotags/health"><kbd class="item-tag">health</kbd></a>
    
    <a href="https://afnan.iotags/data"><kbd class="item-tag">data</kbd></a>
    
    <a href="https://afnan.iotags/science"><kbd class="item-tag">science</kbd></a>
    

</div>


    <br> <div class="text-left"><p>We will examine an unlabeled dataset of recorded data about livers that are believed to be indicative of liver disorders/liver disease, as well as the frequency of drinks per day per person to create a model. We will use K-means clustering to find interesting groups/clusters within the dataset. We will also use cross validation and ensemble learning to fine-tune the model.</p>

<h2 id="exploring-the-data">Exploring the data</h2>

<p>We will examine the traits of the liver dataset so we can understand the relationships in the data and understand the shape of the dataset.</p>

<p>We will begin by importing all of the appropriate Python libraries.</p>

<pre><code class="language-python">import pandas as pd
import sklearn as sk
import numpy as np
import matplotlib as plt
from sklearn.preprocessing import normalize, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import KFold, GridSearchCV
from sklearn.metrics import silhouette_score
</code></pre>

<p>Now we will load our <code>csv</code> file into a Pandas Dataframe so we can manipulate and read the data. We will load the data and preview the data.</p>

<p>The structure of the data is as follows (also found <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/liver-disorders/bupa.names">here</a>)</p>

<ol>
<li>mcv: mean corpuscular volume</li>
<li>alkphos: alkaline phosphotase</li>
<li>sgpt: alamine aminotransferase</li>
<li>sgot: aspartate aminotransferase</li>
<li>gammagt: gamma-glutamyl transpeptidase</li>
<li>drinks: number of half-pint equivalents of alcoholic beverages drunk per day</li>
<li>selector field used to split data into two sets</li>
</ol>

<p>It appears that column 7 is arbitrary.</p>

<p>We can explore the relationship between the data and the number of drinks to see if there's some sort of polynomial relationship or heavy clustering. Right now, we will decide that the number of drinks, whether as some threshold, or some polynomial relationship, is our <code>y</code> value</p>

<pre><code class="language-python">col_names = [
    &quot;mcv&quot;,
    &quot;alkphos&quot;,
    &quot;sgpt&quot;,
    &quot;sgot&quot;,
    &quot;gammagt&quot;,
    &quot;drinks&quot;,
    &quot;gt5&quot;,  # &quot;greater than 5&quot;, this is the selector as described above
]
raw_data = pd.read_csv(&quot;data/liver_data.csv&quot;, header=None, names=col_names)
</code></pre>

<p>Let's look at the data's attributes. We will start by looking at the first 5 entries.</p>

<pre><code class="language-python">print(raw_data.head(5))
</code></pre>

<pre><code>   mcv  alkphos  sgpt  sgot  gammagt  drinks  gt5
0   85       92    45    27       31     0.0    1
1   85       64    59    32       23     0.0    2
2   86       54    33    16       54     0.0    2
3   91       78    34    24       36     0.0    2
4   87       70    12    28       10     0.0    2
</code></pre>

<p>The selector column is useless for our purposes, so we will delete that column from the dataset. If we want to split the data, we can do it ourselves with a random split or with <code>scikit-learn</code>'s K-fold cross validation functions. We will then look at the dataframe's metadata.</p>

<pre><code class="language-python">del raw_data[&quot;gt5&quot;]
raw_data.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 345 entries, 0 to 344
Data columns (total 6 columns):
mcv        345 non-null int64
alkphos    345 non-null int64
sgpt       345 non-null int64
sgot       345 non-null int64
gammagt    345 non-null int64
drinks     345 non-null float64
dtypes: float64(1), int64(5)
memory usage: 16.2 KB
</code></pre>

<p>We will now normalize the data with <code>L1</code> normalization. Having different scales for different features can bias the machine learning model and it is generally better to have normalized data for the sake of computation.</p>

<p>We will also make the assumption that the traits present in the dataset have a distribution close to the normal distribution amongst the general population.</p>

<pre><code class="language-python">norm_data = normalize(raw_data, norm=&quot;l1&quot;, axis=0)
norm_data = pd.DataFrame(columns=raw_data.columns, data=norm_data)

scaler = MinMaxScaler()
m_norm_data = normalize(raw_data, norm=&quot;max&quot;, axis=0)
m_norm_data = pd.DataFrame(columns=raw_data.columns, data=m_norm_data)  # convert back to a dataframe
norm_data[&quot;drinks&quot;] = m_norm_data[&quot;drinks&quot;]
</code></pre>

<p>Scikit-learn returned a <code>numpy</code> array. We will convert it back to a Pandas Dataframe and bring back the column headers found in <code>raw_data</code>, since they were removed by the normalization function.</p>

<pre><code class="language-python">norm_data = pd.DataFrame(columns=raw_data.columns, data=norm_data)

print(norm_data.head(5))
print(norm_data.info())
</code></pre>

<pre><code>        mcv   alkphos      sgpt      sgot   gammagt  drinks
0  0.002733  0.003817  0.004290  0.003176  0.002347     0.0
1  0.002733  0.002655  0.005624  0.003764  0.001741     0.0
2  0.002765  0.002240  0.003146  0.001882  0.004088     0.0
3  0.002926  0.003236  0.003241  0.002823  0.002726     0.0
4  0.002797  0.002904  0.001144  0.003293  0.000757     0.0
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 345 entries, 0 to 344
Data columns (total 6 columns):
mcv        345 non-null float64
alkphos    345 non-null float64
sgpt       345 non-null float64
sgot       345 non-null float64
gammagt    345 non-null float64
drinks     345 non-null float64
dtypes: float64(6)
memory usage: 16.2 KB
None
</code></pre>

<p>Let's create a correlation matrix and see the correlation values between each attribute</p>

<pre><code class="language-python">corr_matrix = norm_data.corr()
print(corr_matrix)
</code></pre>

<pre><code>              mcv   alkphos      sgpt      sgot   gammagt    drinks
mcv      1.000000  0.044103  0.147695  0.187765  0.222314  0.312680
alkphos  0.044103  1.000000  0.076208  0.146057  0.133140  0.100796
sgpt     0.147695  0.076208  1.000000  0.739675  0.503435  0.206848
sgot     0.187765  0.146057  0.739675  1.000000  0.527626  0.279588
gammagt  0.222314  0.133140  0.503435  0.527626  1.000000  0.341224
drinks   0.312680  0.100796  0.206848  0.279588  0.341224  1.000000
</code></pre>

<h2 id="generating-a-model">Generating a model</h2>

<p>We can see that the <code>drinks</code> column doesn't have a strong correlation with any of the other data points. Regressions will probably not provide good results. We can try to cluster the data into two different groups with K-means clustering using k-fold cross validation, and see how effectively it divides the dataset into groups. We will try several different hyperparameters using <code>GridSearchCV</code> in <code>scikit-learn</code> to find the best model via ensemble learning.</p>

<p>We will first configure the cross validation split. We are going to use K-fold cross validation with random shuffling (since the data is sorted by drinks). Since we only have 345 data entries, we will keep <code>k</code> small, and set <code>k=3</code>. We set the random entropy seed so we yield the same results every time this model is generated.</p>

<pre><code class="language-python">RAND_STATE=50  # for reproducibility and consistency
folds=3
k_fold = KFold(n_splits=folds, shuffle=True, random_state=RAND_STATE)  # setting generator for k-fold splitting
</code></pre>

<p>Now we will define the hyperparameters we want iterate through in order to find the best model</p>

<pre><code class="language-python"># Dictionary of hyperparameters to iterate through
# GridSearchCV will try every combination of these hyperparameters and
# return the model with the best score via KFold validation
hyperparams = {
    &quot;n_clusters&quot;: [2, 3],
    &quot;n_init&quot;: [10, 15, 20],
    &quot;max_iter&quot;: [100, 200, 300, 400, 500],
    &quot;tol&quot;: [.0000001, .000001, .00001, .0001],
}

k_means = KMeans()  # sets jobs equal to number of cores

ensemble = GridSearchCV(
    estimator=k_means,
    param_grid=hyperparams,
    cv=k_fold,
    n_jobs=-1
)
</code></pre>

<p>Now we will fit the estimator to our data employing the methods discussed above.</p>

<pre><code class="language-python">ensemble.fit(norm_data)
</code></pre>

<pre><code>GridSearchCV(cv=KFold(n_splits=3, random_state=50, shuffle=True),
       error_score='raise',
       estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
    n_clusters=8, n_init=10, n_jobs=1, precompute_distances='auto',
    random_state=None, tol=0.0001, verbose=0),
       fit_params=None, iid=True, n_jobs=-1,
       param_grid={'max_iter': [100, 200, 300, 400, 500], 'n_init': [10, 15, 20], 'tol': [1e-07, 1e-06, 1e-05, 0.0001], 'n_clusters': [2, 3]},
       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
       scoring=None, verbose=0)
</code></pre>

<p>Now, we need to evaluate the effectiveness of the clustering model. This is difficult to do because the <code>score()</code> doesn't provide a good metric, since there is no &quot;ground truth&quot; for the model, since it's unlabeled. We can look at the silhoutte score, which shows how close the points are to the center of their clusters (tighter clusters will give us a better score, if the data points are very scattered, this indicates that our clusters are too loose). The values for the score range from <code>[-1, 1]</code> and a score of <code>1</code> is ideal.</p>

<pre><code class="language-python"># Generate labels for data with model with raw data, compute score
labels = ensemble.predict(norm_data)
score = silhouette_score(norm_data, labels)
print(score)
print(ensemble.best_params_)
</code></pre>

<pre><code>0.67850734737
{'max_iter': 100, 'n_init': 10, 'tol': 1e-07, 'n_clusters': 3}
</code></pre>

<p>Now we have split our data into clusters, without any knowledge of what the data actually signifies--we don't know what these groups actually consitute. We tried the model with 20 clusters, and got a very good score, but that is probably overfitting and not very generalization (since we only have 345 samples), so we shaved off some of the more extreme possibilities with the grid search, slowly paring down the number of groups as to prevent overfitting and to keep the model useful and generalizable. This left us with the parameters listed above.</p>

<p>Without having any labels, we were able to deduce three interesting groups within our data with a fairly high silhouette score.</p>

<p>If we want to turn this into a binary classification (say to diagnose those with normal livers and those that don't), we simply set <code>n_clusters=2</code> to try to divide the data into 2 groups. Let's do that below.</p>

<pre><code class="language-python"># hyperparameters to try out for binary classification with KNN
bin_params = {
    &quot;n_init&quot;: [10, 15, 20],
    &quot;max_iter&quot;: [100, 200, 300, 400, 500],
    &quot;tol&quot;: [.0000001, .000001, .00001, .0001],
}

bin_k_means = KMeans(n_clusters=2)  # set 2 clusters

binary_ensemble = GridSearchCV(
    estimator=bin_k_means,
    param_grid=bin_params,
    cv=k_fold,
    n_jobs=-1
)

binary_ensemble.fit(norm_data)  # fit model to data and return best model

# Generate labels for data with model with raw data, compute score
bin_labels = binary_ensemble.predict(norm_data)
bin_score = silhouette_score(norm_data, bin_labels)

# Output score to user
print(bin_score)
print(binary_ensemble.best_params_)
</code></pre>

<pre><code>0.639223346286
{'max_iter': 100, 'tol': 1e-07, 'n_init': 10}
</code></pre>

<p>We can see that binary classification has a pretty decent score that isn't too much lower than when we divided the data into three groups.</p>
</div>

    
    

    

        <h4 class="page-header">Related</h4>

         <div class="item">

    
    
    

    
      

    <h3><a href="/post/2017-09-28-explaining-gradient-descent/">Explaining Gradient Descent</a></h3>
    <h4>A cursory explanation of gradient descent and stochastic gradient descent</h4>
    
    <a href="https://afnan.iotags/machine"><kbd class="item-tag">machine</kbd></a>
    
    <a href="https://afnan.iotags/learning"><kbd class="item-tag">learning</kbd></a>
    
    <a href="https://afnan.iotags/gradient"><kbd class="item-tag">gradient</kbd></a>
    
    <a href="https://afnan.iotags/descent"><kbd class="item-tag">descent</kbd></a>
    
    <a href="https://afnan.iotags/loss"><kbd class="item-tag">loss</kbd></a>
    
    <a href="https://afnan.iotags/function"><kbd class="item-tag">function</kbd></a>
    
    <a href="https://afnan.iotags/math"><kbd class="item-tag">math</kbd></a>
    

</div>
 

    

    

        <h4 class="page-header">Comments</h4>

        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "afnanenayet" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    

</main>

        <footer>

            <p class="copyright text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a></p>

        </footer>
       
    </body>

</html>

