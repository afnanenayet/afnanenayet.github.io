<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><meta name=description content="A personal site"><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400" rel=stylesheet type=text/css><link rel=icon type=image/png href=/favicon_16x16.png sizes=16x16><link rel=icon type=image/png href=/favicon_32x32.png sizes=32x32><link rel=icon type=image/png href=/favicon_128x128.png sizes=128x128><link rel=stylesheet href=/style.css><title>Diagnosing livers with machine learning</title><link rel=canonical href=https://afnan.io/posts/2017-10-31-using-k-means-clustering-in-scikit-learn/></head><body><section id=nav><h1><a href=/>Afnan Enayet</a></h1><ul><li><a href=/about/>About</a></li><li><a href=/project/>Projects</a></li><li><a href=https://github.com/afnanenayet>Github</a></li></ul></section><section id=content><h1>Diagnosing livers with machine learning</h1><div id=sub-header>October 2017 Â· 7 minute read</div><div class=entry-content><p>We will examine an unlabeled dataset of recorded data about livers that are believed to be indicative of liver disorders/liver disease, as well as the frequency of drinks per day per person to create a model. We will use K-means clustering to find interesting groups/clusters within the dataset. We will also use cross validation and ensemble learning to fine-tune the model.</p><h2 id=exploring-the-data>Exploring the data</h2><p>We will examine the traits of the liver dataset so we can understand the relationships in the data and understand the shape of the dataset.</p><p>We will begin by importing all of the appropriate Python libraries.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>pandas</span> <span class=kn>as</span> <span class=nn>pd</span>
<span class=kn>import</span> <span class=nn>sklearn</span> <span class=kn>as</span> <span class=nn>sk</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>matplotlib</span> <span class=kn>as</span> <span class=nn>plt</span>
<span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>normalize</span><span class=p>,</span> <span class=n>MinMaxScaler</span>
<span class=kn>from</span> <span class=nn>sklearn.cluster</span> <span class=kn>import</span> <span class=n>KMeans</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>KFold</span><span class=p>,</span> <span class=n>GridSearchCV</span>
<span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>silhouette_score</span></code></pre></div><p>Now we will load our <code>csv</code> file into a Pandas Dataframe so we can manipulate and read the data. We will load the data and preview the data.</p><p>The structure of the data is as follows (also found <a href=http://archive.ics.uci.edu/ml/machine-learning-databases/liver-disorders/bupa.names>here</a>)</p><ol><li>mcv: mean corpuscular volume</li><li>alkphos: alkaline phosphotase</li><li>sgpt: alamine aminotransferase</li><li>sgot: aspartate aminotransferase</li><li>gammagt: gamma-glutamyl transpeptidase</li><li>drinks: number of half-pint equivalents of alcoholic beverages drunk per day</li><li>selector field used to split data into two sets</li></ol><p>It appears that column 7 is arbitrary.</p><p>We can explore the relationship between the data and the number of drinks to see if there's some sort of polynomial relationship or heavy clustering. Right now, we will decide that the number of drinks, whether as some threshold, or some polynomial relationship, is our <code>y</code> value</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>col_names</span> <span class=o>=</span> <span class=p>[</span>
    <span class=s2>&#34;mcv&#34;</span><span class=p>,</span>
    <span class=s2>&#34;alkphos&#34;</span><span class=p>,</span>
    <span class=s2>&#34;sgpt&#34;</span><span class=p>,</span>
    <span class=s2>&#34;sgot&#34;</span><span class=p>,</span>
    <span class=s2>&#34;gammagt&#34;</span><span class=p>,</span>
    <span class=s2>&#34;drinks&#34;</span><span class=p>,</span>
    <span class=s2>&#34;gt5&#34;</span><span class=p>,</span>  <span class=c1># &#34;greater than 5&#34;, this is the selector as described above</span>
<span class=p>]</span>
<span class=n>raw_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&#34;data/liver_data.csv&#34;</span><span class=p>,</span> <span class=n>header</span><span class=o>=</span><span class=bp>None</span><span class=p>,</span> <span class=n>names</span><span class=o>=</span><span class=n>col_names</span><span class=p>)</span></code></pre></div><p>Let's look at the data's attributes. We will start by looking at the first 5 entries.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>print</span><span class=p>(</span><span class=n>raw_data</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>5</span><span class=p>))</span></code></pre></div><pre><code>   mcv  alkphos  sgpt  sgot  gammagt  drinks  gt5
0   85       92    45    27       31     0.0    1
1   85       64    59    32       23     0.0    2
2   86       54    33    16       54     0.0    2
3   91       78    34    24       36     0.0    2
4   87       70    12    28       10     0.0    2</code></pre><p>The selector column is useless for our purposes, so we will delete that column from the dataset. If we want to split the data, we can do it ourselves with a random split or with <code>scikit-learn</code>'s K-fold cross validation functions. We will then look at the dataframe's metadata.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>del</span> <span class=n>raw_data</span><span class=p>[</span><span class=s2>&#34;gt5&#34;</span><span class=p>]</span>
<span class=n>raw_data</span><span class=o>.</span><span class=n>info</span><span class=p>()</span></code></pre></div><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 345 entries, 0 to 344
Data columns (total 6 columns):
mcv        345 non-null int64
alkphos    345 non-null int64
sgpt       345 non-null int64
sgot       345 non-null int64
gammagt    345 non-null int64
drinks     345 non-null float64
dtypes: float64(1), int64(5)
memory usage: 16.2 KB</code></pre><p>We will now normalize the data with <code>L1</code> normalization. Having different scales for different features can bias the machine learning model and it is generally better to have normalized data for the sake of computation.</p><p>We will also make the assumption that the traits present in the dataset have a distribution close to the normal distribution amongst the general population.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>norm_data</span> <span class=o>=</span> <span class=n>normalize</span><span class=p>(</span><span class=n>raw_data</span><span class=p>,</span> <span class=n>norm</span><span class=o>=</span><span class=s2>&#34;l1&#34;</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=n>norm_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>columns</span><span class=o>=</span><span class=n>raw_data</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span><span class=n>norm_data</span><span class=p>)</span>

<span class=n>scaler</span> <span class=o>=</span> <span class=n>MinMaxScaler</span><span class=p>()</span>
<span class=n>m_norm_data</span> <span class=o>=</span> <span class=n>normalize</span><span class=p>(</span><span class=n>raw_data</span><span class=p>,</span> <span class=n>norm</span><span class=o>=</span><span class=s2>&#34;max&#34;</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=n>m_norm_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>columns</span><span class=o>=</span><span class=n>raw_data</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span><span class=n>m_norm_data</span><span class=p>)</span>  <span class=c1># convert back to a dataframe</span>
<span class=n>norm_data</span><span class=p>[</span><span class=s2>&#34;drinks&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>m_norm_data</span><span class=p>[</span><span class=s2>&#34;drinks&#34;</span><span class=p>]</span></code></pre></div><p>Scikit-learn returned a <code>numpy</code> array. We will convert it back to a Pandas Dataframe and bring back the column headers found in <code>raw_data</code>, since they were removed by the normalization function.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>norm_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>columns</span><span class=o>=</span><span class=n>raw_data</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span><span class=n>norm_data</span><span class=p>)</span>

<span class=k>print</span><span class=p>(</span><span class=n>norm_data</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>5</span><span class=p>))</span>
<span class=k>print</span><span class=p>(</span><span class=n>norm_data</span><span class=o>.</span><span class=n>info</span><span class=p>())</span></code></pre></div><pre><code>        mcv   alkphos      sgpt      sgot   gammagt  drinks
0  0.002733  0.003817  0.004290  0.003176  0.002347     0.0
1  0.002733  0.002655  0.005624  0.003764  0.001741     0.0
2  0.002765  0.002240  0.003146  0.001882  0.004088     0.0
3  0.002926  0.003236  0.003241  0.002823  0.002726     0.0
4  0.002797  0.002904  0.001144  0.003293  0.000757     0.0
&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 345 entries, 0 to 344
Data columns (total 6 columns):
mcv        345 non-null float64
alkphos    345 non-null float64
sgpt       345 non-null float64
sgot       345 non-null float64
gammagt    345 non-null float64
drinks     345 non-null float64
dtypes: float64(6)
memory usage: 16.2 KB
None</code></pre><p>Let's create a correlation matrix and see the correlation values between each attribute</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>corr_matrix</span> <span class=o>=</span> <span class=n>norm_data</span><span class=o>.</span><span class=n>corr</span><span class=p>()</span>
<span class=k>print</span><span class=p>(</span><span class=n>corr_matrix</span><span class=p>)</span></code></pre></div><pre><code>              mcv   alkphos      sgpt      sgot   gammagt    drinks
mcv      1.000000  0.044103  0.147695  0.187765  0.222314  0.312680
alkphos  0.044103  1.000000  0.076208  0.146057  0.133140  0.100796
sgpt     0.147695  0.076208  1.000000  0.739675  0.503435  0.206848
sgot     0.187765  0.146057  0.739675  1.000000  0.527626  0.279588
gammagt  0.222314  0.133140  0.503435  0.527626  1.000000  0.341224
drinks   0.312680  0.100796  0.206848  0.279588  0.341224  1.000000</code></pre><h2 id=generating-a-model>Generating a model</h2><p>We can see that the <code>drinks</code> column doesn't have a strong correlation with any of the other data points. Regressions will probably not provide good results. We can try to cluster the data into two different groups with K-means clustering using k-fold cross validation, and see how effectively it divides the dataset into groups. We will try several different hyperparameters using <code>GridSearchCV</code> in <code>scikit-learn</code> to find the best model via ensemble learning.</p><p>We will first configure the cross validation split. We are going to use K-fold cross validation with random shuffling (since the data is sorted by drinks). Since we only have 345 data entries, we will keep <code>k</code> small, and set <code>k=3</code>. We set the random entropy seed so we yield the same results every time this model is generated.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>RAND_STATE</span><span class=o>=</span><span class=mi>50</span>  <span class=c1># for reproducibility and consistency</span>
<span class=n>folds</span><span class=o>=</span><span class=mi>3</span>
<span class=n>k_fold</span> <span class=o>=</span> <span class=n>KFold</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=n>folds</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=bp>True</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=n>RAND_STATE</span><span class=p>)</span>  <span class=c1># setting generator for k-fold splitting</span></code></pre></div><p>Now we will define the hyperparameters we want iterate through in order to find the best model</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Dictionary of hyperparameters to iterate through</span>
<span class=c1># GridSearchCV will try every combination of these hyperparameters and</span>
<span class=c1># return the model with the best score via KFold validation</span>
<span class=n>hyperparams</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s2>&#34;n_clusters&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span>
    <span class=s2>&#34;n_init&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>20</span><span class=p>],</span>
    <span class=s2>&#34;max_iter&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>300</span><span class=p>,</span> <span class=mi>400</span><span class=p>,</span> <span class=mi>500</span><span class=p>],</span>
    <span class=s2>&#34;tol&#34;</span><span class=p>:</span> <span class=p>[</span><span class=o>.</span><span class=mo>0000001</span><span class=p>,</span> <span class=o>.</span><span class=mo>000001</span><span class=p>,</span> <span class=o>.</span><span class=mo>00001</span><span class=p>,</span> <span class=o>.</span><span class=mo>0001</span><span class=p>],</span>
<span class=p>}</span>

<span class=n>k_means</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>()</span>  <span class=c1># sets jobs equal to number of cores</span>

<span class=n>ensemble</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span>
    <span class=n>estimator</span><span class=o>=</span><span class=n>k_means</span><span class=p>,</span>
    <span class=n>param_grid</span><span class=o>=</span><span class=n>hyperparams</span><span class=p>,</span>
    <span class=n>cv</span><span class=o>=</span><span class=n>k_fold</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span>
<span class=p>)</span></code></pre></div><p>Now we will fit the estimator to our data employing the methods discussed above.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>ensemble</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>norm_data</span><span class=p>)</span></code></pre></div><pre><code>GridSearchCV(cv=KFold(n_splits=3, random_state=50, shuffle=True),
       error_score=&#39;raise&#39;,
       estimator=KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300,
    n_clusters=8, n_init=10, n_jobs=1, precompute_distances=&#39;auto&#39;,
    random_state=None, tol=0.0001, verbose=0),
       fit_params=None, iid=True, n_jobs=-1,
       param_grid={&#39;max_iter&#39;: [100, 200, 300, 400, 500], &#39;n_init&#39;: [10, 15, 20], &#39;tol&#39;: [1e-07, 1e-06, 1e-05, 0.0001], &#39;n_clusters&#39;: [2, 3]},
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=True,
       scoring=None, verbose=0)</code></pre><p>Now, we need to evaluate the effectiveness of the clustering model. This is difficult to do because the <code>score()</code> doesn't provide a good metric, since there is no "ground truth" for the model, since it's unlabeled. We can look at the silhoutte score, which shows how close the points are to the center of their clusters (tighter clusters will give us a better score, if the data points are very scattered, this indicates that our clusters are too loose). The values for the score range from <code>[-1, 1]</code> and a score of <code>1</code> is ideal.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Generate labels for data with model with raw data, compute score</span>
<span class=n>labels</span> <span class=o>=</span> <span class=n>ensemble</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>norm_data</span><span class=p>)</span>
<span class=n>score</span> <span class=o>=</span> <span class=n>silhouette_score</span><span class=p>(</span><span class=n>norm_data</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>ensemble</span><span class=o>.</span><span class=n>best_params_</span><span class=p>)</span></code></pre></div><pre><code>0.67850734737
{&#39;max_iter&#39;: 100, &#39;n_init&#39;: 10, &#39;tol&#39;: 1e-07, &#39;n_clusters&#39;: 3}</code></pre><p>Now we have split our data into clusters, without any knowledge of what the data actually signifies--we don't know what these groups actually consitute. We tried the model with 20 clusters, and got a very good score, but that is probably overfitting and not very generalization (since we only have 345 samples), so we shaved off some of the more extreme possibilities with the grid search, slowly paring down the number of groups as to prevent overfitting and to keep the model useful and generalizable. This left us with the parameters listed above.</p><p>Without having any labels, we were able to deduce three interesting groups within our data with a fairly high silhouette score.</p><p>If we want to turn this into a binary classification (say to diagnose those with normal livers and those that don't), we simply set <code>n_clusters=2</code> to try to divide the data into 2 groups. Let's do that below.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># hyperparameters to try out for binary classification with KNN</span>
<span class=n>bin_params</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s2>&#34;n_init&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>20</span><span class=p>],</span>
    <span class=s2>&#34;max_iter&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>300</span><span class=p>,</span> <span class=mi>400</span><span class=p>,</span> <span class=mi>500</span><span class=p>],</span>
    <span class=s2>&#34;tol&#34;</span><span class=p>:</span> <span class=p>[</span><span class=o>.</span><span class=mo>0000001</span><span class=p>,</span> <span class=o>.</span><span class=mo>000001</span><span class=p>,</span> <span class=o>.</span><span class=mo>00001</span><span class=p>,</span> <span class=o>.</span><span class=mo>0001</span><span class=p>],</span>
<span class=p>}</span>

<span class=n>bin_k_means</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>  <span class=c1># set 2 clusters</span>

<span class=n>binary_ensemble</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span>
    <span class=n>estimator</span><span class=o>=</span><span class=n>bin_k_means</span><span class=p>,</span>
    <span class=n>param_grid</span><span class=o>=</span><span class=n>bin_params</span><span class=p>,</span>
    <span class=n>cv</span><span class=o>=</span><span class=n>k_fold</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span>
<span class=p>)</span>

<span class=n>binary_ensemble</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>norm_data</span><span class=p>)</span>  <span class=c1># fit model to data and return best model</span>

<span class=c1># Generate labels for data with model with raw data, compute score</span>
<span class=n>bin_labels</span> <span class=o>=</span> <span class=n>binary_ensemble</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>norm_data</span><span class=p>)</span>
<span class=n>bin_score</span> <span class=o>=</span> <span class=n>silhouette_score</span><span class=p>(</span><span class=n>norm_data</span><span class=p>,</span> <span class=n>bin_labels</span><span class=p>)</span>

<span class=c1># Output score to user</span>
<span class=k>print</span><span class=p>(</span><span class=n>bin_score</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>binary_ensemble</span><span class=o>.</span><span class=n>best_params_</span><span class=p>)</span></code></pre></div><pre><code>0.639223346286
{&#39;max_iter&#39;: 100, &#39;tol&#39;: 1e-07, &#39;n_init&#39;: 10}</code></pre><p>We can see that binary classification has a pretty decent score that isn't too much lower than when we divided the data into three groups.</p></div></section></body></html>