<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="chrome=1">
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="referrer" content="no-referrer">
        <meta name="description" content="A personal site">

        
            <link href='https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400' rel='stylesheet' type='text/css'>
        

        <link rel="icon" type="image/png" href="/favicon_16x16.png" sizes="16x16">
        <link rel="icon" type="image/png" href="/favicon_32x32.png" sizes="32x32">
        <link rel="icon" type="image/png" href="/favicon_128x128.png" sizes="128x128">

        <title>
            
            
                 Explaining Gradient Descent 
                
        </title>
        <link rel="canonical" href="https://afnan.io/posts/2017-09-28-explaining-gradient-descent/">

        <style>
    * {
        border: 0;
        font: inherit;
        font-size: 100%;
        vertical-align: baseline;
        margin: 0;
        padding: 0;
        color: black;
        text-decoration-skip: ink;
    }

    @font-face {
        font-family: 'Open Sans', sans-serif;
        font-display: auto;
        src: local('Open Sans'), url('https://fonts.googleapis.com/css?family=Open+Sans&display=swap');
    }

    body {
        font-family: 'Open Sans', 'Myriad Pro', Myriad, sans-serif;
        font-size: 17px;
        line-height: 160%;
        color: #1d1313;
        max-width: 700px;
        margin: auto;
    }

    p {
        margin: 20px 0;
    }

    a img {
        border: none;
    }

    img {
        margin: 10px auto 10px auto;
        max-width: 100%;
        display: block;
    }

    .left-justify {
        float: left;
    }

    .right-justify {
        float: right;
    }

    pre,
    code {
        font: Consolas, "Liberation Mono", Menlo, Courier, monospace;
        background-color: #f7f7f7;
    }

    code {
        padding: 2px;
    }

    pre {
        margin-top: 0;
        margin-bottom: 16px;
        word-wrap: normal;
        padding: 16px;
        overflow: auto;
        font-size: 85%;
        line-height: 1.45;
    }

    pre>code {
        padding: 0;
        margin: 0;
        font-size: 100%;
        word-break: normal;
        white-space: pre;
        background: transparent;
        border: 0;
    }

    pre code {
        display: inline;
        max-width: auto;
        padding: 0;
        margin: 0;
        overflow: visible;
        line-height: inherit;
        word-wrap: normal;
        background-color: transparent;
        border: 0;
    }

    pre code::before,
    pre code::after {
        content: normal;
    }

    em,
    q,
    em,
    dfn {
        font-style: italic;
    }

    .sans,
    html .gist .gist-file .gist-meta {
        font-family: "Open Sans", "Myriad Pro", Myriad, sans-serif;
    }

    .mono,
    pre,
    code,
    tt,
    p code,
    li code {
        font-family: Menlo, Monaco, "Andale Mono", "lucida console", "Courier New", monospace;
    }

    .serif {
        font-family: "Old Standard TT", serif;
    }

    strong {
        font-weight: 600;
    }

    q:before {
        content: "\201C";
    }

    q:after {
        content: "\201D";
    }

    del,
    s {
        text-decoration: line-through;
    }

    blockquote {
        font-family: "Old Standard TT", serif;
        text-align: center;
        padding: 50px;
    }

    blockquote p {
        display: inline-block;
        font-style: italic;
    }

    blockquote:before,
    blockquote:after {
        font-family: "Old Standard TT", serif;
        content: '\201C';
        font-size: 35px;
        color: #403c3b;
    }

    blockquote:after {
        content: '\201D';
    }

    hr {
        width: 40%;
        height: 1px;
        background: #403c3b;
        margin: 25px auto;
    }

    h1 {
        font-size: 35px;
    }

    h2 {
        font-size: 28px;
    }

    h3 {
        font-size: 22px;
        margin-top: 18px;
    }

    h1 a,
    h2 a,
    h3 a {
        text-decoration: none;
    }

    h1,
    h2 {
        margin-top: 28px;
    }

    #sub-header,
    time {
        color: #403c3b;
        font-size: 13px;
    }

    #sub-header {
        margin: 0 4px;
    }

    #nav h1 a {
        font-size: 35px;
        color: #1d1313;
        line-height: 120%;
    }

    .posts_listing a,
    #nav a {
        text-decoration: none;
    }

    li {
        margin-left: 20px;
    }

    ul li {
        margin-left: 5px;
    }

    ul li {
        list-style-type: none;
    }

    ul li:before {
        content: "\00BB \0020";
    }

    #nav ul li:before,
    .posts_listing li:before {
        content: '';
        margin-right: 0;
    }

    #content {
        text-align: left;
        width: 100%;
        font-size: 15px;
        padding: 60px 0 80px;
    }

    #content h1,
    #content h2 {
        margin-bottom: 5px;
    }

    #content h2 {
        font-size: 25px;
    }

    #content .entry-content {
        margin-top: 15px;
    }

    #content time {
        margin-left: 3px;
    }

    #content h1 {
        font-size: 30px;
    }

    .highlight {
        margin: 10px 0;
    }

    .posts_listing {
        margin: 0 0 50px;
    }

    .posts_listing li {
        margin: 0 0 25px 15px;
    }

    .posts_listing li a:hover,
    #nav a:hover {
        text-decoration: underline;
    }

    #nav {
        text-align: center;
        position: static;
        margin-top: 60px;
    }

    #nav ul {
        display: table;
        margin: 8px auto 0 auto;
    }

    #nav li {
        list-style-type: none;
        display: table-cell;
        font-size: 15px;
        padding: 0 20px;
    }

    #links {
        margin: 50px 0 0 0;
    }

    #links :nth-child(2) {
        float: right;
    }

    #not-found {
        text-align: center;
    }

    #not-found a {
        font-family: "Old Standard TT", serif;
        font-size: 200px;
        text-decoration: none;
        display: inline-block;
        padding-top: 225px;
    }

    @media (max-width: 750px) {
        body {
            padding-left: 20px;
            padding-right: 20px;
        }

        #nav h1 a {
            font-size: 28px;
        }

        #nav li {
            font-size: 13px;
            padding: 0 15px;
        }

        #content {
            margin-top: 0;
            padding-top: 50px;
            font-size: 14px;
        }

        #content h1 {
            font-size: 25px;
        }

        #content h2 {
            font-size: 22px;
        }

        .posts_listing li div {
            font-size: 12px;
        }
    }

    @media (max-width: 400px) {
        body {
            padding-left: 20px;
            padding-right: 20px;
        }

        #nav h1 a {
            font-size: 22px;
        }

        #nav li {
            font-size: 12px;
            padding: 0 10px;
        }

        #content {
            margin-top: 0;
            padding-top: 20px;
            font-size: 14px;
        }

        #content h1 {
            font-size: 20px;
        }

        #content h2 {
            font-size: 18px;
        }

        .posts_listing li div {
            font-size: 12px;
        }
    }

    @media (prefers-color-scheme: dark) {

        *,
        #nav h1 a {
            color: #FDFDFD;
        }

        body {
            background: #121212;
        }

        pre,
        code {
            background-color: #262626;
        }

        #sub-header,
        time {
            color: #BABABA;
        }

        hr {
            background: #EBEBEB;
        }

        .highlight>pre {
            background-color: #262626;
        }

        highlight>pre {
            background-color: #262626;
        }
    }

     
    .chroma {
        background-color: #f7f7f7
    }

    @media (prefers-color-scheme: dark) {
        .chroma {
            background-color: #262626
        }
    }

     
    .chroma .lntd {
        vertical-align: top;
        padding: 0;
        margin: 0;
        border: 0;
    }

     
    .chroma .lntable {
        border-spacing: 0;
        padding: 0;
        margin: 0;
        border: 0;
        width: auto;
        overflow: auto;
        display: block;
    }

     
    .chroma .hl {
        display: block;
        width: 100%;
        background-color: #ffffcc
    }

     
    .chroma .lnt {
        margin-right: 0.4em;
        padding: 0 0.4em 0 0.4em;
        color: #7f7f7f
    }

     
    .chroma .ln {
        margin-right: 0.4em;
        padding: 0 0.4em 0 0.4em;
        color: #7f7f7f
    }

     
    .chroma .k {
        font-weight: bold
    }

     
    .chroma .kc {
        font-weight: bold
    }

     
    .chroma .kd {
        font-weight: bold
    }

     
    .chroma .kn {
        font-weight: bold
    }

     
    .chroma .kr {
        font-weight: bold
    }

     
    .chroma .nc {
        font-weight: bold
    }

     
    .chroma .ni {
        font-weight: bold
    }

     
    .chroma .ne {
        font-weight: bold
    }

     
    .chroma .nn {
        font-weight: bold
    }

     
    .chroma .nt {
        font-weight: bold
    }

     
    .chroma .sa {
        font-style: italic
    }

     
    .chroma .sb {
        font-style: italic
    }

     
    .chroma .sc {
        font-style: italic
    }

     
    .chroma .dl {
        font-style: italic
    }

     
    .chroma .sd {
        font-style: italic
    }

     
    .chroma .s2 {
        font-style: italic
    }

     
    .chroma .se {
        font-weight: bold;
    }

     
    .chroma .sh {
        font-style: italic
    }

     
    .chroma .si {
        font-weight: bold;
        font-style: italic
    }

     
    .chroma .sx {
        font-style: italic
    }

     
    .chroma .sr {
        font-style: italic
    }

     
    .chroma .s1 {
        font-style: italic
    }

     
    .chroma .ss {
        font-style: italic
    }

     
    .chroma .ow {
        font-weight: bold
    }

     
    .chroma .c {
        font-style: italic
    }

     
    .chroma .ch {
        font-style: italic
    }

     
    .chroma .cm {
        font-style: italic
    }

     
    .chroma .c1 {
        font-style: italic
    }

     
    .chroma .cs {
        font-style: italic
    }

     
    .chroma .ge {
        font-style: italic
    }

     
    .chroma .gh {
        font-weight: bold
    }

     
    .chroma .gp {
        font-weight: bold
    }

     
    .chroma .gs {
        font-weight: bold
    }

     
    .chroma .gu {
        font-weight: bold
    }
</style>


        

        
            
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>


        
    </head>

    <body>
        <section id=nav>
            <h1><a href="/">Afnan Enayet</a></h1>
            <ul>
                
                    <li><a href="/">Home</a></li>
                
                    <li><a href="/about/">About</a></li>
                
                    <li><a href="/">Posts</a></li>
                
                    <li><a href="/project/">Projects</a></li>
                
            </ul>
        </section>


<section id=content>
  <h1> Explaining Gradient Descent </h1>

  
    <div id=sub-header>
      September 2017 · 6 minute read
    </div>
  

  <div class="entry-content">
    <h2 id="a-classic-statistics-problem">A classic statistics problem</h2>

<p>In machine learning, problems are often framed as optimization problems. For example,
let us take one of the simplest applications of supervised learning: the linear
regression. It's conceptually an easy problem - given a set of data points,
create a function <span  class="math">\( f = ax + b \)</span> that best approximates these points. If you
ever took statistics or have used excel, this is the line of best fit.
Conceptually, this operation doesn't seem too difficult. Note that this is a
supervised learning problem - we need to have a set of observed data to &quot;train&quot;
from. We use this data to create the line of best fit, which we can then subsequently
use to make predictions given new parameters.</p>

<h2 id="framing-it-as-a-machine-learning-problem">Framing it as a machine learning problem</h2>

<h3 id="defining-a-linear-regression">Defining a linear regression</h3>

<p>Let's rewrite the equation <span  class="math">\( f(x) = ax + b \)</span> as <span  class="math">\( f(x) = a_1 x + a_0 \)</span>. We have
two weights that we need to define, <span  class="math">\( a_0 \)</span> and <span  class="math">\( a_1 \)</span>, that correspond
to the features <span  class="math">\( x_0 \)</span> and <span  class="math">\( x_1 \)</span>. <span  class="math">\( x_0 = 1 \)</span> and is kind of a
&quot;dummy variable&quot; because we need to fit a linear regression which has the
offset <span  class="math">\( a_0 \)</span>, there isn't any actual observation associated with <span  class="math">\( x_0 \)</span>
and it has no measured effect on the final line of best fit.</p>

<p>Our data is defined as a matrix <span  class="math">\( X \)</span> where each row of the matrix represents
the readings for the <span  class="math">\( i^{th} \)</span> observation. Each row, <span  class="math">\( x^{(i)T} \)</span>, is the
set of observed variables for a particular observation, which means it must have
our feature and the observed output. Note that it is transposed because <span  class="math">\( x^{(i)} \)</span>
is a vector with the recorded features for that observation. We have another vector
<span  class="math">\( Y \)</span> that holds the actual observations for <span  class="math">\( X \)</span>, so the <span  class="math">\( i^{th} \)</span> element
in <span  class="math">\( Y \)</span> corresponds to the <span  class="math">\( i^{th} \)</span> row of <span  class="math">\( X \)</span>.</p>

<p>Our objective is to create some function <span  class="math">\( f \)</span> in the form <span  class="math">\( f(x) = a*0 + a_1 x \)</span>
that best approximates the data in <span  class="math">\( X \)</span> and <span  class="math">\( Y \)</span>. This means that we need to
pick <span  class="math">\( a_0, a_1 \)</span> such that <span  class="math">\( f \)</span> best approximates _all* of the data in <span  class="math">\( X \)</span>.
So how do we pick the best <span  class="math">\( a_0, a_1 \)</span>?</p>

<h3 id="the-loss-function">The loss function</h3>

<p>Suppose we randomly guess <span  class="math">\( a_0, a_1 \)</span>. We want to know how far off from the actual
data we are, we need a quantitative way to determine how wrong we are. We can measure
the &quot;wrongness&quot; of our function with something called a loss function. A loss function
is a function that provides a measure of your wrongness, and our objective is to
minimize it as best as possible. There are several loss functions you can use, a very
popular loss function is called mean-squared error (MSE).</p>

<p>We define <span  class="math">\( \text{MSE} \)</span> as follows:</p>

<p><span  class="math">\[
\text{MSE} = \frac{1}{n} \sum\_{i = 1}^n (y^{(i)} - f(x^{(i)}))^2
\]</span></p>

<p>In essence, we are taking every row in <span  class="math">\( X \)</span>, making a prediciton with our function
<span  class="math">\( f = a_0 + a_1 x \)</span>, taking the difference between the actual <span  class="math">\( y \)</span> value and
our prediction <span  class="math">\( f(x^{(i)}) \)</span>, then squaring the difference. We do this for every
row, then we divide by <span  class="math">\( n \)</span> (the number of data points we have). This gives us
the average squared error in the data set, and lets us quantify how good our
line of best fit is. Of course, we want to minimize our loss function.</p>

<p>We can find the minimum of a quadratic function by finding the root of the
derivation. Note that it has to be a concave down function, otherwise the
root will give us the maximum.</p>

<p>Let us define our loss function <span  class="math">\( L(a) \)</span> as <span  class="math">\( MSE \)</span> and find the minimum.</p>

<p><span  class="math">\[
\begin{aligned}
L(a) &= \frac{1}{n} \sum*{i = 1}^n (y^{(i)} - f(x^{(i)}))^2 \\
L(a) &= \frac{1}{n} \sum*{i = 1}^n (y^{(i)} - (a^T x^{(i)})^2 \\
L'(a) &= \frac{1}{n} _ 2 (y^{(i)} - a^T x^{(i)}) _ -x^{(i)} \\
L'(a) &= -\frac{2}{n} (y^{(i)} - a^T x^{(i)}) x^{(i)} \\
0 &= -\frac{2}{n} (y^{(i)} - a^T x^{(i)}) x^{(i)}
\end{aligned}
\]</span></p>

<p>We have our derived function, solving for <span  class="math">\( a \)</span> will be left as an
exercise to the reader.</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p>With our defined loss function, how do we figure out how to guess <span  class="math">\( a \)</span> in
an iterative manner? We could start with some random vector for <span  class="math">\( a \)</span> and
then increment the variables a little, trying to lower <span  class="math">\( L(a) \)</span> with every
step until we converge on the minimum (so <span  class="math">\( L(a) \)</span> cannot be minimized
any further). This is the intuition behind gradient descent, except
gradient descent has a clever way of incrementing the new <span  class="math">\( a \)</span> values so
that every time gradient descent iterates, <span  class="math">\( L(a) \)</span> is guaranteed to
decrease.</p>

<p>In gradient descent, take our vector <span  class="math">\( a*t \)</span> where <span  class="math">\( t \)</span> is our current
iteration. With each iteration of gradient descent, we set <span  class="math">\( a*{t+1} \)</span>
equal to some new value as defined below:</p>

<p><span  class="math">\[
\begin{aligned}
a*t &= a*{t-1} - \alpha f(a\_{t-1})
\end{aligned}
\]</span></p>

<p>Gradient descent allows us to iteratively minimize the loss function - in other
words, this is how we find the value with the least amount of error. Intuitively,
imagine a ball rolling down a hill. The hill is drawn by the loss function, and
we want to reach the bottom. Because we are subtracting by a ratio defined by
the slope of the hill, with a convex function, we should be able to optimize
and find the minimum. If we don't have a strictly concave down function,
there is a chance we will not be able to find the minimum. Additionally,
note the <span  class="math">\( \alpha \)</span> term. This is the step size, this is a constant that
we define which will affect how fast our gradient descent mechanism rolls
the ball down the hill. If we keep the step size small, it could take a while
until we reach the bottom of the hill, it might also get stuck in a local minima.
If the step size is too large, it may oscillate around the minimum but never
quite converge.</p>

<h3 id="optimize-for-big-data-stochastic-gradient-descent">Optimize for big data (stochastic gradient descent)</h3>

<p>Gradient descent can be an expensive process - imagine our <span  class="math">\( n \)</span> is extremely
large (this is the definition of big data). If <span  class="math">\( n \)</span> is really big, then
every iteration of gradient descent will be extremely expensive. In practice,
we must balance the accuracy of our models with computation time. A lot of
models that use massive amounts of data mitigate this with a loss function
called stochastic gradient descent.</p>

<p>The main intuition behind stochastic gradient descent is that the model is
trained and tested on a small subset of the data at a time. This way, you
get a rougher approximation of the optimal value(s), but it is much faster.
Because gradient descent is an approximation itself, the penalty in accuracy
brought on by stochastic gradient descent isn't big enough to outweigh the
performance gains that are brought on by operating on only a batch of the
data at a time.</p>

<p>If you're using stochastic gradient descent, you'll want to randomly shuffle
the data, and perform each iteration on a randomly shuffled mini-batch.</p>

  </div>

  <div id=links>
    
      <a class="basic-alignment left" href="https://afnan.io/posts/2017-09-27-exposing-terminal-variables-with-c/">&laquo; Exposing terminal variables with C</a>
    
    
      <a class="basic-alignment left" href="https://afnan.io/posts/2017-10-31-using-k-means-clustering-in-scikit-learn/">Diagnosing livers with machine learning &raquo;</a>
    
  </div>
</section>

    
</body>
</html>

